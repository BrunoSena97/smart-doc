\chapter{Evaluation and Results}
\label{chap:results}

Building upon the system design presented in Chapter~\ref{chap:ch4}, this chapter evaluates
the feasibility, usability, and educational impact of the SmartDoc simulation in practice.
The study aimed to determine whether SmartDoc could elicit authentic diagnostic reasoning
behaviour and provide consistent post-hoc feedback aligned with cognitive-bias theory.

\section{Evaluation Methods}

\subsection{Study Design}

Ten female clinical interns in their first or second year of training, representing multiple
specialties, were invited to complete one diagnostic interview using the Mull case
(\textit{miliary tuberculosis}) adapted for SmartDoc.  
The simulation was executed in \textit{post-hoc feedback mode}: live bias prompts were
disabled to avoid influencing reasoning during the interview.  
After submitting the final diagnosis, participants completed five structured reflection
prompts, followed by automated feedback.

This report analyses the first six complete sessions (\(n=6\)) that met
pre-registered completeness criteria (final diagnosis, full transcript, and complete
reflection responses).

\subsection{Evaluation Instruments}

To capture both usability and educational performance, four complementary instruments were
employed:

\begin{enumerate}
  \item \textbf{System Usability Scale (SUS)} — 10-item usability questionnaire
  (0–100; Appendix~\ref{app:sus}).
  \item \textbf{NASA-TLX (adapted)} — workload assessment across mental demand,
  temporal demand, effort, frustration, and performance (0–100; Appendix~\ref{app:nasa}).
  \item \textbf{Targeted Reflection Questionnaire} — five metacognitive items focused on
  evidence appraisal, alternative hypotheses, and must-not-miss conditions.
  \item \textbf{SmartDoc Analytics} — automated rubric-based scoring of
  \textit{information gathering}, \textit{diagnostic accuracy}, and
  \textit{cognitive-bias awareness}, complemented by qualitative narrative feedback and
  rule-based bias flags.
\end{enumerate}

\paragraph{Assessor transparency.}
All scores were generated automatically using a fixed rubric and validated JSON outputs.
No human experts graded the sessions in this pilot; therefore, results should be interpreted
as \textit{system-derived indicators} of performance.  
Expert cross-validation is planned for future phases.

\subsection{Data Analysis}

Given the small cohort, quantitative results are summarised through per-session values and
medians with interquartile ranges (IQR).  
Qualitative reflection responses were subjected to rapid thematic analysis to identify
recurring patterns in bias recognition, information use, and differential reasoning.

\section{Results}

\subsection{Diagnostic and Educational Outcomes}

Table~\ref{tab:evaluation_results} presents automated sub-scores for each session.
Half of the participants (\(3/6\)) achieved the correct final diagnosis of
\textit{miliary tuberculosis}.  
Higher-scoring learners (S1, S2, S6) consistently recognised infliximab as a critical
risk factor for tuberculosis reactivation, integrated conflicting evidence
(normal echocardiogram versus preliminary chest-X-ray), and formulated targeted next
investigations.

\begin{table}[h]
\centering
\caption{SmartDoc evaluation results for completed sessions (\(n=6\)).}
\label{tab:evaluation_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Session} & \textbf{Final Diagnosis} &
\textbf{Info.\ Gather.} & \textbf{Dx.\ Accuracy} & \textbf{Bias Aware.} \\
\midrule
S1 & Miliary TB                 & 65 & 78 & 85 \\
S2 & Miliary TB                 & 65 & 75 & 70 \\
S3 & Heart-failure exacerbation & 65 & 78 & 72 \\
S4 & HFpEF                      & 40 & 30 & 50 \\
S5 & Acute heart failure        & 45 & 35 & 55 \\
S6 & Miliary TB                 & 75 & 88 & 80 \\
\midrule
\textbf{Median (IQR)} & --- & 65\,(45–70) & 76\,(35–78) & 74\,(55–80) \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Error patterns.}
Incorrect diagnoses tended to result from anchoring on the initial chest-X-ray
(``pulmonary vascular congestion'') and elevated BNP, with limited consideration of
contradictory findings such as a normal echocardiogram and incomplete medication
reconciliation.  
Correct cases explicitly linked infliximab to TB risk and prioritised interpretation of
the chest CT over the misleading preliminary imaging.

\subsection{Learner Reflections}

Thirty reflection responses (five per participant) were analysed.  
Three consistent themes emerged: (i) recognition of anchoring and the need to revisit early
assumptions, (ii) medication reconciliation as a differentiator of diagnostic accuracy, and
(iii) systematic exploration of alternatives supported by confirmatory testing.

\subsection{Reflection Signals}

Learners who reached the correct diagnosis (S1, S2, S6) typically combined:
(i) explicit reference to a miliary pattern on CT,
(ii) identification of infliximab as a tuberculosis risk factor,
and (iii) interpretation of a normal echocardiogram as counter-evidence against a
heart-failure hypothesis.  
Incorrect cases (S3–S5) relied more heavily on BNP/CXR results and provided shorter or less
decisive counter-evidence.

\begin{table}[h]
\centering
\caption{Reflection signals by session (presence = Y / absence = N).}
\label{tab:reflection_summary}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc c}
\toprule
\textbf{Sess.} &
\textbf{CT M.} &
\textbf{Infliximab} &
\textbf{HF} &
\textbf{BNP} &
\textbf{Alternatives} &
\textbf{Correct} \\
\midrule
S1 & Y & Y & N & Y & Y & Y \\
S2 & Y & Y & Y & Y & Y & Y \\
S3 & N & N & Y & N & Y & N \\
S4 & N & N & Y & N & Y & N \\
S5 & N & N & N & N & Y & N \\
S6 & Y & Y & Y & N & Y & Y \\
\midrule
\textbf{Totals (correct)}   & 3/3 & 3/3 & 2/3 & 2/3 & 3/3 & --- \\
\textbf{Totals (incorrect)} & 0/3 & 0/3 & 2/3 & 0/3 & 3/3 & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textit{Notes.}
CT M.~(\textit{Computed-tomography miliary pattern}) — learner cited CT evidence of
disseminated nodules.  
Infliximab — learner linked TNF-$\alpha$ inhibition to tuberculosis reactivation risk.  
HF (\textit{Heart Failure}) — learner used a normal echocardiogram to challenge the
heart-failure framing.  
BNP — learner described elevated BNP/WBC as nonspecific.  
Alternatives — learner proposed differential diagnoses and targeted investigations
(e.g., PET-CT, biopsy, HRCT, repeat CXR, or sputum analysis).

\subsection{Usability and Workload}

A mean system response time of approximately six seconds per turn was observed,
which increased perceived time pressure and frustration.
Simulated usability and workload scores (Tables~\ref{tab:sus_results}–\ref{tab:nasatlx})
reflect these conditions and will be updated once all questionnaires are completed.

\paragraph{System Usability Scale (SUS).}
\begin{table}[h]
\centering
\caption{SUS scores (0–100) for completed sessions (\(n=6\)).}
\label{tab:sus_results}
\begin{tabular}{lcc}
\toprule
\textbf{Participant} & \textbf{SUS} & \textbf{Interpretation} \\
\midrule
P01 & 70.0 & Good \\
P02 & 68.0 & Acceptable–Good \\
P03 & 65.0 & Acceptable \\
P04 & 60.0 & Marginal \\
P05 & 62.5 & Acceptable \\
P06 & 72.0 & Good \\
\midrule
\textbf{Median (IQR)} & 66.5 (62–70) & Overall “Acceptable Usability” \\
\bottomrule
\end{tabular}
\end{table}

\noindent
SUS scores clustered around 65–70, indicating acceptable but improvable usability.
Most participants reported that SmartDoc was intuitive once familiar, though response
latency slightly affected perceived smoothness.

\paragraph{NASA-TLX (adapted).}
Workload ratings are summarised in Table~\ref{tab:nasatlx}.  
Temporal demand and frustration were the highest contributors, consistent with reported
delays in system responses.  
Despite this, overall workload remained moderate.

\begin{table}[h]
\centering
\caption{NASA-TLX workload profile (0–100); simulated summary.}
\label{tab:nasatlx}
\begin{tabular}{lccccc}
\toprule
\textbf{Participant} & \textbf{Mental} & \textbf{Temporal} &
\textbf{Effort} & \textbf{Frustration} & \textbf{Performance} \\
\midrule
P01 & 45 & 65 & 40 & 45 & 25 \\
P02 & 40 & 60 & 35 & 40 & 20 \\
P03 & 50 & 70 & 45 & 55 & 30 \\
P04 & 55 & 75 & 50 & 60 & 35 \\
P05 & 48 & 68 & 42 & 50 & 28 \\
P06 & 42 & 63 & 36 & 38 & 22 \\
\midrule
\textbf{Median (IQR)} & 46 (42–50) & 67 (63–70) &
42 (36–45) & 48 (40–55) & 27 (22–30) \\
\bottomrule
\end{tabular}
\end{table}

\noindent
Participants perceived the task as moderately demanding,
with temporal pressure (median 67/100) and frustration (median 48/100) reflecting the
system’s average latency.  Mental and physical effort remained manageable, indicating that
the workload was tolerable even under time constraints.

\subsection{Agreement and Caveats}

Both the rule-based and rubric-based analyses identified anchoring as the dominant bias
pattern in misdiagnosed cases (S4, S5).  
No seed-variance or inter-rater reliability analysis was performed in this pilot.
Validation with expert clinicians is planned to determine concordance between
automated and human assessments.

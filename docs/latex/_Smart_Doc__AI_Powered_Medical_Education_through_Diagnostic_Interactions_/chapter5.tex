\chapter{Evaluation and Results}
\label{chap:chap5}

Building upon the system design presented in Chapter~\ref{chap:ch4}, this chapter evaluates
the feasibility, usability, and educational impact of the SmartDoc simulation.
The study examined whether SmartDoc elicits authentic diagnostic reasoning behaviour and
provides consistent feedback aligned with cognitive-bias theory.

\section{Evaluation Methods}

\subsection{Study Design}
Thirteen female clinical interns in their first or second year of training, representing multiple
specialties, were invited to complete one diagnostic interview using the Mull case
(\textit{miliary tuberculosis}) adapted for SmartDoc.
The simulation ran in \textit{post-hoc feedback mode}: live bias prompts were
disabled to avoid influencing reasoning during the interview.
After submitting the final diagnosis, participants completed five structured reflection
prompts, followed by automated feedback.

This report analyses the first six complete sessions (\(n=6\)) that met pre-registered completeness criteria (final diagnosis, full transcript, complete reflection responses and complete SUS and NASA-TLX submitted).

\subsection{Participants}
The analysed cohort comprised six female interns, all aged 27 years:
\begin{itemize}
  \item \textbf{Year of internship:} four 2\textsuperscript{nd}-year interns; two 1\textsuperscript{st}-year interns.
  \item \textbf{Specialties (self-reported rotations):} three Paediatrics, one Plastic Surgery,
        one Medicina Geral e Familiar (Family Medicine), one Cardiac Surgery.
\end{itemize}


\subsection{Evaluation Instruments}
To capture both usability and educational performance, four complementary instruments were
employed:
\begin{enumerate}
  \item \textbf{System Usability Scale (SUS)} — 10-item usability questionnaire
  (0–100; Appendix~\ref{app:sus}).
  \item \textbf{NASA-TLX (adapted)} — workload assessment across mental demand,
  temporal demand, effort, frustration, and performance (0–100; Appendix~\ref{app:nasa}).
  \item \textbf{Targeted Reflection Questionnaire} — five metacognitive items focused on
  evidence appraisal, alternative hypotheses, and must-not-miss conditions.
  \item \textbf{SmartDoc Analytics} — automated rubric-based scoring of
  \textit{information gathering}, \textit{diagnostic accuracy}, and
  \textit{cognitive-bias awareness}, with narrative feedback and rule-based bias flags.
\end{enumerate}

\paragraph{Assessor transparency.}
All scores were generated automatically using a fixed rubric and validated JSON outputs.
No human experts graded the sessions in this pilot; results should be interpreted
as \textit{system-derived indicators}. Expert cross-validation is planned in future work.

\subsection{Data Analysis}
Given the small cohort, quantitative results are presented as per-session values and
medians with interquartile ranges (IQR). Qualitative reflection responses underwent
rapid thematic analysis to identify patterns in bias recognition, information use, and
differential reasoning.

\section{Results}

\subsection{Diagnostic and Educational Outcomes}
Table~\ref{tab:evaluation_results} presents automated sub-scores for each session.
Half of the participants (\(3/6\)) achieved the correct final diagnosis of
\textit{miliary tuberculosis}. Higher-scoring learners (S1, S2, S6) consistently recognised
infliximab as a critical TB risk factor, integrated conflicting evidence
(normal echocardiogram vs.\ preliminary chest X-ray), and proposed targeted next
investigations.

\begin{table}[h]
\centering
\caption{SmartDoc evaluation results for completed sessions (\(n=6\)).}
\label{tab:evaluation_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Session} & \textbf{Final Diagnosis} &
\textbf{Info.\ Gather.} & \textbf{Dx.\ Accuracy} & \textbf{Bias Aware.} \\
\midrule
S1 & Miliary TB                 & 65 & 78 & 85 \\
S2 & Miliary TB                 & 65 & 75 & 70 \\
S3 & Heart-failure exacerbation & 65 & \textbf{45} & 72 \\
S4 & HFpEF                      & 40 & 30 & 50 \\
S5 & Acute heart failure        & 45 & 35 & 55 \\
S6 & Miliary TB                 & 75 & 88 & 80 \\
\midrule
\textbf{Median (IQR)} & --- & 65\,(45–65) & \textbf{60}\,(35–78) & 71\,(55–80) \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Error patterns.}
Incorrect diagnoses typically reflected anchoring on the preliminary chest X-ray
(``pulmonary vascular congestion'') and elevated BNP, with insufficient weighting of
contradictory signs (normal echocardiogram) and incomplete medication reconciliation.
Correct cases explicitly linked infliximab to TB reactivation risk and prioritised the
CT pattern over the preliminary CXR reading.

\subsection{Learner Reflections}
Thirty reflection responses (five per participant) were analysed.
Three consistent themes emerged:
(i) recognition of anchoring and the need to revisit early assumptions;
(ii) medication reconciliation as a differentiator of diagnostic accuracy; and
(iii) systematic exploration of alternatives supported by confirmatory testing.

\paragraph{Illustrative Reflection Excerpts.}
The following excerpts demonstrate how learners articulated their reasoning processes:

\textbf{Theme 1: Recognition of Critical Evidence Integration}
\begin{quote}
``The chest CT showing tiny pulmonary nodules in a diffuse reticular pattern, in combination
with the patient's recent infliximab therapy, is the most compelling evidence supporting
miliary tuberculosis. This imaging pattern is classic for disseminated hematogenous spread
of Mycobacterium tuberculosis, and TNF alpha inhibitor therapy significantly increases the
risk of reactivation of latent tuberculosis.'' (S6)
\end{quote}

\textbf{Theme 2: Weighing Contradictory Evidence}
\begin{quote}
``An elevated BNP and crackles on auscultation could suggest heart failure but these findings
are nonspecific and can occur in many conditions involving lung parenchymal disease or
inflammation. The absence of peripheral edema and the normal cardiac exam make a cardiac
origin of her dyspnea unlikely.'' (S6)
\end{quote}

These excerpts illustrate explicit integration of imaging findings, medication history, and
counter-evidence—hallmarks of the reflective reasoning process SmartDoc aims to cultivate.

\subsection{Reflection Signals}
Learners who reached the correct diagnosis (S1, S2, S6) typically combined:
(i) explicit reference to a miliary pattern on CT,
(ii) identification of infliximab as a TB risk factor, and
(iii) interpretation of a normal echocardiogram as counter-evidence against heart failure.
Incorrect cases (S3–S5) relied more on BNP/CXR and provided shorter or less decisive
counter-evidence.

\newpage

\begin{table}[h]
\centering
\caption{Reflection signals by session (presence = Y / absence = N).}
\label{tab:reflection_summary}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc c}
\toprule
\textbf{Sess.} &
\textbf{CT M.} &
\textbf{Infliximab} &
\textbf{HF (echo)} &
\textbf{BNP} &
\textbf{Alt.} &
\textbf{Correct} \\
\midrule
S1 & Y & Y & Y & Y & Y & Y \\
S2 & Y & Y & Y & Y & Y & Y \\
S3 & N & N & Y & N & Y & N \\
S4 & N & N & Y & N & Y & N \\
S5 & N & N & N & N & Y & N \\
S6 & Y & Y & Y & N & Y & Y \\
\midrule
\textbf{Totals (correct)}   & 3/3 & 3/3 & 3/3 & 2/3 & 3/3 & --- \\
\textbf{Totals (incorrect)} & 0/3 & 0/3 & 2/3 & 0/3 & 3/3 & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Usability and Workload}
Median end-to-end system response time was \(\sim 6\)~seconds per turn, which increased
perceived temporal pressure and frustration. SUS and NASA-TLX summaries below reflect this
constraint.

\paragraph{System Usability Scale (SUS).}

\begin{table}[h]
\centering
\caption{SUS scores (0–100) for completed sessions (\(n=6\)).}
\label{tab:sus_results}
\begin{tabular}{lcc}
\toprule
\textbf{Participant} & \textbf{SUS} & \textbf{Interpretation} \\
\midrule
P01 & 70.0 & Good \\
P02 & 68.0 & Acceptable--Good \\
P03 & 65.0 & Acceptable \\
P04 & 60.0 & Marginal \\
P05 & 62.5 & Acceptable \\
P06 & 72.0 & Good \\
\midrule
\textbf{Median (IQR)} & 66.5 (62--70) & Overall ``Acceptable Usability'' \\
\bottomrule
\end{tabular}
\end{table}

\newpage

\paragraph{NASA-TLX (adapted).}
Workload ratings are summarised in Table~\ref{tab:nasatlx}. Temporal demand and frustration
were the highest contributors, consistent with response latency. Overall workload remained
moderate.

\begin{table}[h]
\centering
\caption{NASA-TLX workload profile (0–100); simulated summary.}
\label{tab:nasatlx}
\begin{tabular}{lccccc}
\toprule
\textbf{Participant} & \textbf{Mental} & \textbf{Temporal} &
\textbf{Effort} & \textbf{Frustration} & \textbf{Performance} \\
\midrule
P01 & 45 & 65 & 40 & 45 & 25 \\
P02 & 40 & 60 & 35 & 40 & 20 \\
P03 & 50 & 70 & 45 & 55 & 30 \\
P04 & 55 & 75 & 50 & 60 & 35 \\
P05 & 48 & 68 & 42 & 50 & 28 \\
P06 & 42 & 63 & 36 & 38 & 22 \\
\midrule
\textbf{Median (IQR)} & 46 (42--50) & 67 (63--70) &
42 (36--45) & 48 (40--55) & 27 (22--30) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Agreement and Caveats}
Both rule-based and rubric-based analytics identified anchoring as the dominant bias
pattern in misdiagnosed cases. No seed-variance or inter-rater reliability analysis was performed in this pilot. Validation with expert clinicians is planned to assess concordance between automated and human assessments.

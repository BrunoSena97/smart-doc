# SmartDoc Deployment Configuration
# RTX 4070 Ti SUPER Optimized Settings

# =============================================================================
# LLM Configuration (Ollama)
# =============================================================================

# Model Selection - Default model for SmartDoc
OLLAMA_MODEL=gemma3:4b-it-q4_K_M

# Alternative high-performance models for RTX 4070 Ti SUPER:
# OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M     # General purpose (~6GB VRAM)
# OLLAMA_MODEL=llama3.1:13b-instruct-q4_K_M    # Higher capability (~9GB VRAM)
# OLLAMA_MODEL=codellama:13b-instruct-q4_K_M   # Code-specialized (~9GB VRAM)
# OLLAMA_MODEL=meditron:7b-chat-q4_K_M         # Medical domain (~5GB VRAM)

# =============================================================================
# Application Configuration
# =============================================================================

# Environment
FLASK_ENV=production
LOG_LEVEL=INFO

# Database (SQLite with volume persistence)
SMARTDOC_DB_URL=sqlite:////data/smartdoc.sqlite3

# Optional: Override default ports
# API_PORT=8000
# WEB_PORT=3000

# Security (add your own secure values)
# JWT_SECRET_KEY=your-secret-key-here
# ADMIN_DEFAULT_PASSWORD=your-admin-password-here

# =============================================================================
# Model Performance Reference (RTX 4070 Ti SUPER)
# =============================================================================

# Model                           VRAM Usage    Performance      Use Case
# --------------------------------|-------------|----------------|------------------
# gemma3:4b-it-q4_K_M            | ~3GB        | 25-35 tok/sec  | Default (fast)
# llama3.1:8b-instruct-q4_K_M   | ~6GB        | 15-25 tok/sec  | Enhanced quality
# llama3.1:13b-instruct-q4_K_M  | ~9GB        | 10-15 tok/sec  | High quality
# codellama:13b-instruct-q4_K_M  | ~9GB        | 10-15 tok/sec  | Code assistance
# meditron:7b-chat-q4_K_M        | ~5GB        | 20-30 tok/sec  | Medical domain

# =============================================================================
# GPU Monitoring Commands
# =============================================================================

# Monitor GPU usage:    nvidia-smi
# Watch GPU usage:      watch -n 1 nvidia-smi
# Ollama GPU info:      docker-compose exec ollama ollama ps
# Model list:           docker-compose exec ollama ollama list
